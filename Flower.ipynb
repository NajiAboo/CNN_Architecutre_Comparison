{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "employed-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fantastic-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "defined-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "configured-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models  import Sequential\n",
    "from keras.layers  import Conv2D, Flatten, MaxPooling2D, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "precise-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path ='//media//sfm//62E4D583E4D559BD6//Users//Naji//Tutorials//dataset//Flowers//flowers'\n",
    "input_shape = (64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "turkish-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "extended-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, kernel_size=(3,3),input_shape=input_shape, activation='relu', padding='same'))\n",
    "classifier.add(Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "classifier.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "classifier.add(Dropout(0.25))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "classifier.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(128, kernel_size=(4,4), activation='relu'))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dense(units=256, activation='relu'))\n",
    "classifier.add(Dense(units=5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "based-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 5, 5, 128)         262272    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               409728    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 993,317\n",
      "Trainable params: 993,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "sized-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cardiovascular-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.3\n",
    "                                  )\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "streaming-peripheral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3024 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set =  datagen.flow_from_directory(dataset_path, target_size=(64,64), batch_size = 16, class_mode='categorical',  subset='training' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "accomplished-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n"
     ]
    }
   ],
   "source": [
    "print(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "comfortable-boring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3024"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "another-sword",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1293 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_set = datagen.flow_from_directory(dataset_path,target_size=(64,64), batch_size=16, class_mode='categorical', subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "later-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 3024//16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "virgin-phrase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "nasty-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.3570 - accuracy: 0.3839WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n",
      "189/189 [==============================] - 66s 349ms/step - loss: 1.3570 - accuracy: 0.3839 - val_loss: 1.2701 - val_accuracy: 0.4323\n",
      "Epoch 2/50\n",
      "189/189 [==============================] - 81s 430ms/step - loss: 1.2539 - accuracy: 0.4560\n",
      "Epoch 3/50\n",
      "189/189 [==============================] - 73s 388ms/step - loss: 1.1669 - accuracy: 0.5198\n",
      "Epoch 4/50\n",
      "189/189 [==============================] - 59s 313ms/step - loss: 1.1139 - accuracy: 0.5463\n",
      "Epoch 5/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 1.0843 - accuracy: 0.5731\n",
      "Epoch 6/50\n",
      "189/189 [==============================] - 61s 320ms/step - loss: 0.9971 - accuracy: 0.6118\n",
      "Epoch 7/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.9584 - accuracy: 0.6267\n",
      "Epoch 8/50\n",
      "189/189 [==============================] - 59s 315ms/step - loss: 0.9324 - accuracy: 0.6505\n",
      "Epoch 9/50\n",
      "189/189 [==============================] - 59s 313ms/step - loss: 0.8637 - accuracy: 0.6819\n",
      "Epoch 10/50\n",
      "189/189 [==============================] - 58s 309ms/step - loss: 0.8246 - accuracy: 0.6915\n",
      "Epoch 11/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.8100 - accuracy: 0.6925\n",
      "Epoch 12/50\n",
      "189/189 [==============================] - 59s 309ms/step - loss: 0.7671 - accuracy: 0.7123\n",
      "Epoch 13/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.7429 - accuracy: 0.7255\n",
      "Epoch 14/50\n",
      "189/189 [==============================] - 60s 316ms/step - loss: 0.7101 - accuracy: 0.7364\n",
      "Epoch 15/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.6787 - accuracy: 0.7447\n",
      "Epoch 16/50\n",
      "189/189 [==============================] - 59s 312ms/step - loss: 0.6741 - accuracy: 0.7404\n",
      "Epoch 17/50\n",
      "189/189 [==============================] - 59s 312ms/step - loss: 0.6352 - accuracy: 0.7649\n",
      "Epoch 18/50\n",
      "189/189 [==============================] - 59s 312ms/step - loss: 0.5987 - accuracy: 0.7758\n",
      "Epoch 19/50\n",
      "189/189 [==============================] - 62s 327ms/step - loss: 0.5687 - accuracy: 0.7917\n",
      "Epoch 20/50\n",
      "189/189 [==============================] - 58s 309ms/step - loss: 0.5565 - accuracy: 0.7923\n",
      "Epoch 21/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.5324 - accuracy: 0.8019\n",
      "Epoch 22/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.5177 - accuracy: 0.8059\n",
      "Epoch 23/50\n",
      "189/189 [==============================] - 59s 311ms/step - loss: 0.4993 - accuracy: 0.8115\n",
      "Epoch 24/50\n",
      "189/189 [==============================] - 59s 313ms/step - loss: 0.4895 - accuracy: 0.8228\n",
      "Epoch 25/50\n",
      "189/189 [==============================] - 59s 311ms/step - loss: 0.4798 - accuracy: 0.8132\n",
      "Epoch 26/50\n",
      "189/189 [==============================] - 60s 317ms/step - loss: 0.4267 - accuracy: 0.8399\n",
      "Epoch 27/50\n",
      "189/189 [==============================] - 58s 307ms/step - loss: 0.4218 - accuracy: 0.8472\n",
      "Epoch 28/50\n",
      "189/189 [==============================] - 59s 312ms/step - loss: 0.4291 - accuracy: 0.8439\n",
      "Epoch 29/50\n",
      "189/189 [==============================] - 59s 313ms/step - loss: 0.3857 - accuracy: 0.8528\n",
      "Epoch 30/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.3526 - accuracy: 0.8690\n",
      "Epoch 31/50\n",
      "189/189 [==============================] - 59s 313ms/step - loss: 0.3733 - accuracy: 0.8611\n",
      "Epoch 32/50\n",
      "189/189 [==============================] - 59s 311ms/step - loss: 0.3454 - accuracy: 0.8760\n",
      "Epoch 33/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.3440 - accuracy: 0.8776\n",
      "Epoch 34/50\n",
      "189/189 [==============================] - 59s 314ms/step - loss: 0.3197 - accuracy: 0.8829\n",
      "Epoch 35/50\n",
      "189/189 [==============================] - 58s 309ms/step - loss: 0.2943 - accuracy: 0.8932\n",
      "Epoch 36/50\n",
      "189/189 [==============================] - 58s 307ms/step - loss: 0.3007 - accuracy: 0.8889\n",
      "Epoch 37/50\n",
      "189/189 [==============================] - 58s 309ms/step - loss: 0.2974 - accuracy: 0.8909\n",
      "Epoch 38/50\n",
      "189/189 [==============================] - 67s 352ms/step - loss: 0.2896 - accuracy: 0.8955\n",
      "Epoch 39/50\n",
      "189/189 [==============================] - 77s 408ms/step - loss: 0.2621 - accuracy: 0.9064\n",
      "Epoch 40/50\n",
      "189/189 [==============================] - 60s 319ms/step - loss: 0.2754 - accuracy: 0.9028\n",
      "Epoch 41/50\n",
      "189/189 [==============================] - 68s 359ms/step - loss: 0.2655 - accuracy: 0.9124\n",
      "Epoch 42/50\n",
      "189/189 [==============================] - 64s 340ms/step - loss: 0.2583 - accuracy: 0.9114\n",
      "Epoch 43/50\n",
      "189/189 [==============================] - 59s 310ms/step - loss: 0.2285 - accuracy: 0.9183\n",
      "Epoch 44/50\n",
      "189/189 [==============================] - 52s 273ms/step - loss: 0.2678 - accuracy: 0.9107\n",
      "Epoch 45/50\n",
      "189/189 [==============================] - 67s 352ms/step - loss: 0.2260 - accuracy: 0.9177\n",
      "Epoch 46/50\n",
      "189/189 [==============================] - 60s 316ms/step - loss: 0.2181 - accuracy: 0.9200\n",
      "Epoch 47/50\n",
      "189/189 [==============================] - 71s 373ms/step - loss: 0.2607 - accuracy: 0.9160\n",
      "Epoch 48/50\n",
      "189/189 [==============================] - 76s 400ms/step - loss: 0.2088 - accuracy: 0.9286\n",
      "Epoch 49/50\n",
      "189/189 [==============================] - 62s 331ms/step - loss: 0.2019 - accuracy: 0.9315\n",
      "Epoch 50/50\n",
      "189/189 [==============================] - 60s 315ms/step - loss: 0.1953 - accuracy: 0.9358\n"
     ]
    }
   ],
   "source": [
    "model = classifier.fit_generator(training_set, steps_per_epoch=steps_per_epoch, epochs=50, validation_data=validation_set, validation_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-minutes",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
